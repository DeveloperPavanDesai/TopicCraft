{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92073674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.13/site-packages (3.8.7)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (2.3.4)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.13/site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.13/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4c6524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbf2a6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Artificial Intelligence is revolutionizing healthcare. \n",
       "It is always beneficial and will completely replace doctors in the future. \n",
       "AI systems are used in diagnosis and treatment planning. \n",
       "Many hospitals are adopting machine learning rapidly."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = \"Artificial Intelligence in Healthcare\"\n",
    "\n",
    "draft_text = \"\"\"\n",
    "Artificial Intelligence is revolutionizing healthcare. \n",
    "It is always beneficial and will completely replace doctors in the future. \n",
    "AI systems are used in diagnosis and treatment planning. \n",
    "Many hospitals are adopting machine learning rapidly.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(draft_text)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "846f6615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 4\n",
      "Words: 38\n",
      "Avg sentence length: 9.5\n",
      "Vocabulary richness: 0.76\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "tokens = [token for token in doc if not token.is_punct]\n",
    "\n",
    "num_sentences = len(sentences)\n",
    "num_words = len(tokens)\n",
    "avg_sentence_length = num_words / num_sentences\n",
    "\n",
    "unique_words = len(set([token.lemma_.lower() for token in tokens]))\n",
    "vocab_richness = unique_words / num_words\n",
    "\n",
    "print(\"Sentences:\", num_sentences)\n",
    "print(\"Words:\", num_words)\n",
    "print(\"Avg sentence length:\", round(avg_sentence_length,2))\n",
    "print(\"Vocabulary richness:\", round(vocab_richness,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6442d996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words\n",
      "[('\\n', 5), ('artificial', 1), ('intelligence', 1), ('revolutionize', 1), ('healthcare', 1), ('beneficial', 1), ('completely', 1), ('replace', 1), ('doctor', 1), ('future', 1)]\n"
     ]
    }
   ],
   "source": [
    "clean_token = [\n",
    "    token.lemma_.lower()\n",
    "    for token in doc\n",
    "    if not token.is_stop and not token.is_punct\n",
    "]\n",
    "\n",
    "word_freq = Counter(clean_token)\n",
    "\n",
    "print(\"Top 10 words\")\n",
    "print(word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb9fdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Distribution:\n",
      "Counter({'NOUN': 10, 'SPACE': 5, 'AUX': 5, 'VERB': 4, 'PUNCT': 4, 'PROPN': 3, 'ADV': 3, 'ADJ': 2, 'CCONJ': 2, 'ADP': 2, 'PRON': 1, 'DET': 1})\n"
     ]
    }
   ],
   "source": [
    "pos_counts = Counter([token.pos_ for token in doc])\n",
    "print(\"POS Distribution:\")\n",
    "print(pos_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f745acf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities:\n",
      "[('Artificial Intelligence', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "print('Entities:')\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "866ae677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Phrases:\n",
      "['\\nArtificial Intelligence', 'healthcare', 'It', 'doctors', 'the future', 'AI systems', 'diagnosis and treatment planning', 'Many hospitals', 'machine learning']\n"
     ]
    }
   ],
   "source": [
    "noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "print(\"Key Phrases:\")\n",
    "print(noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96098b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passive Sentences:\n",
      "['AI systems are used in diagnosis and treatment planning. \\n']\n"
     ]
    }
   ],
   "source": [
    "passive_sentences = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.dep_ == \"nsubjpass\":\n",
    "        passive_sentences.append(token.sent.text)\n",
    "\n",
    "print(\"Passive Sentences:\")\n",
    "print(passive_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6771530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Relevance Score: 0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/s0m504y16rjdvlp6g5cvbh000000gq/T/ipykernel_49367/2564093225.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity_score = topic_doc.similarity(doc)\n"
     ]
    }
   ],
   "source": [
    "topic_doc = nlp(topic)\n",
    "\n",
    "similarity_score = topic_doc.similarity(doc)\n",
    "print(\"Topic Relevance Score:\", round(similarity_score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68f2504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Bias Words:\n",
      "['always', 'completely']\n"
     ]
    }
   ],
   "source": [
    "#Bias Detection\n",
    "bias_words = [\"always\", \"never\", \"completely\", \"disaster\", \"amazing\"]\n",
    "\n",
    "found_bias = [\n",
    "    token.text for token in doc\n",
    "    if token.text.lower() in bias_words\n",
    "]\n",
    "\n",
    "print(\"Potential Bias Words:\")\n",
    "print(found_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48c46515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Quality Score: 80\n"
     ]
    }
   ],
   "source": [
    "score = 100\n",
    "\n",
    "if avg_sentence_length > 25:\n",
    "    score -= 10\n",
    "\n",
    "if len(passive_sentences) > 0:\n",
    "    score -= 10\n",
    "\n",
    "if len(found_bias) > 0:\n",
    "    score -= 10\n",
    "\n",
    "print(\"Writing Quality Score:\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
